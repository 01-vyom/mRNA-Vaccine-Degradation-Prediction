{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-03-23T20:24:38.161087Z","iopub.status.busy":"2022-03-23T20:24:38.160796Z","iopub.status.idle":"2022-03-23T20:24:38.168991Z","shell.execute_reply":"2022-03-23T20:24:38.167918Z","shell.execute_reply.started":"2022-03-23T20:24:38.161042Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import gc\n","import os\n","import matplotlib.pyplot as plt\n","import json\n","from tqdm.notebook import tqdm\n","%matplotlib inline\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-03-23T20:24:40.052652Z","iopub.status.busy":"2022-03-23T20:24:40.052363Z","iopub.status.idle":"2022-03-23T20:24:40.171384Z","shell.execute_reply":"2022-03-23T20:24:40.170551Z","shell.execute_reply.started":"2022-03-23T20:24:40.052619Z"},"trusted":true},"outputs":[],"source":["def get_adj_matrix(data_df):\n","    As = []\n","    for id in tqdm(data_df[\"id\"]):\n","        a = np.load(f\"{path}/bpps/{id}.npy\")\n","        As.append(a)\n","    As = np.array(As)\n","    \n","    ## get adjacent matrix from structure sequence\n","    sequence_structure_adj = []\n","    for i in tqdm(range(len(data_df))):\n","        seq_length = data_df[\"seq_length\"].iloc[i]\n","        structure = data_df[\"structure\"].iloc[i]\n","        sequence = data_df[\"sequence\"].iloc[i]\n","\n","        cue = []\n","        a_structures = {\n","            (\"A\", \"U\") : np.zeros([seq_length, seq_length]),\n","            (\"C\", \"G\") : np.zeros([seq_length, seq_length]),\n","            (\"U\", \"G\") : np.zeros([seq_length, seq_length]),\n","            (\"U\", \"A\") : np.zeros([seq_length, seq_length]),\n","            (\"G\", \"C\") : np.zeros([seq_length, seq_length]),\n","            (\"G\", \"U\") : np.zeros([seq_length, seq_length]),\n","        }\n","        a_structure = np.zeros([seq_length, seq_length])\n","        for i in range(seq_length):\n","            if structure[i] == \"(\":\n","                cue.append(i)\n","            elif structure[i] == \")\":\n","                start = cue.pop()\n","                a_structures[(sequence[start], sequence[i])][start, i] = 1\n","                a_structures[(sequence[i], sequence[start])][i, start] = 1\n","        \n","        a_strc = np.stack([a for a in a_structures.values()], axis = 2)\n","        a_strc = np.sum(a_strc, axis = 2, keepdims = True)\n","        sequence_structure_adj.append(a_strc)\n","    \n","    sequence_structure_adj = np.array(sequence_structure_adj)\n","    print(sequence_structure_adj.shape)\n","    \n","    ## adjacent matrix based on distance on the sequence\n","    ## D[i, j] = 1 / (abs(i - j) + 1) ** pow, pow = 1, 2, 4\n","    idx = np.arange(As.shape[1])\n","    distance_matrix = []\n","    for i in range(len(idx)):\n","        distance_matrix.append(np.abs(idx[i] - idx))\n","\n","    distance_matrix = np.array(distance_matrix) + 1\n","    distance_matrix = 1/distance_matrix\n","    distance_matrix = distance_matrix[None, :,:]\n","    distance_matrix = np.repeat(distance_matrix, len(As), axis = 0)\n","    \n","    Dss = []\n","    for i in [1, 2, 4]: \n","        Dss.append(distance_matrix ** i)\n","    distance_matrix = np.stack(Dss, axis = 3)\n","    print(distance_matrix.shape)\n","    \n","    adjacency_matrix = np.concatenate([As[:,:,:,None], sequence_structure_adj, distance_matrix], axis = 3).astype(np.float32)\n","    \n","    return adjacency_matrix\n","    \n","    "]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-03-23T20:24:44.820248Z","iopub.status.busy":"2022-03-23T20:24:44.819939Z","iopub.status.idle":"2022-03-23T20:24:44.834526Z","shell.execute_reply":"2022-03-23T20:24:44.833589Z","shell.execute_reply.started":"2022-03-23T20:24:44.820214Z"},"trusted":true},"outputs":[],"source":["def get_node_features(train):\n","    ## get node features, which is one hot encoded\n","    mapping = {}\n","    vocab = [\"A\", \"G\", \"C\", \"U\"]\n","    for i, s in enumerate(vocab):\n","        mapping[s] = [0] * len(vocab)\n","        mapping[s][i] = 1\n","    X_node = np.stack(train[\"sequence\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n","\n","    mapping = {}\n","    vocab = [\"S\", \"M\", \"I\", \"B\", \"H\", \"E\", \"X\"]\n","    for i, s in enumerate(vocab):\n","        mapping[s] = [0] * len(vocab)\n","        mapping[s][i] = 1\n","    X_loop = np.stack(train[\"predicted_loop_type\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n","    \n","    mapping = {}\n","    vocab = [\".\", \"(\", \")\"]\n","    for i, s in enumerate(vocab):\n","        mapping[s] = [0] * len(vocab)\n","        mapping[s][i] = 1\n","    X_structure = np.stack(train[\"structure\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n","    \n","    \n","    X_node = np.concatenate([X_node, X_loop], axis = 2)\n","    \n","    ## interaction\n","    a = np.sum(X_node * (2 ** np.arange(X_node.shape[2])[None, None, :]), axis = 2)\n","    vocab = sorted(set(a.flatten()))\n","    print(vocab)\n","    ohes = []\n","    for v in vocab:\n","        ohes.append(a == v)\n","    ohes = np.stack(ohes, axis = 2)\n","    X_node = np.concatenate([X_node, ohes], axis = 2).astype(np.float32)\n","    \n","    \n","    print(X_node.shape)\n","    return X_node\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-03-23T20:24:48.250048Z","iopub.status.busy":"2022-03-23T20:24:48.249753Z","iopub.status.idle":"2022-03-23T20:24:55.098450Z","shell.execute_reply":"2022-03-23T20:24:55.097508Z","shell.execute_reply.started":"2022-03-23T20:24:48.250018Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import layers as L\n","import tensorflow_addons as tfa\n","from tensorflow.keras import backend as K\n","\n","def mcrmse(t, p, seq_len_target):\n","    ## calculate mcrmse score by using numpy\n","    t = t[:, :seq_len_target]\n","    p = p[:, :seq_len_target]\n","    \n","    score = np.mean(np.sqrt(np.mean(np.mean((p - t) ** 2, axis = 1), axis = 0)))\n","    return score\n","\n","def attention(x_inner, x_outer, n_factor, dropout):\n","    x_Q =  L.Conv1D(n_factor, 1, activation='linear', \n","                  kernel_initializer='glorot_uniform',\n","                  bias_initializer='glorot_uniform',\n","                 )(x_inner)\n","    x_K =  L.Conv1D(n_factor, 1, activation='linear', \n","                  kernel_initializer='glorot_uniform',\n","                  bias_initializer='glorot_uniform',\n","                 )(x_outer)\n","    x_V =  L.Conv1D(n_factor, 1, activation='linear', \n","                  kernel_initializer='glorot_uniform',\n","                  bias_initializer='glorot_uniform',\n","                 )(x_outer)\n","    x_KT = L.Permute((2, 1))(x_K)\n","    res = L.Lambda(lambda c: K.batch_dot(c[0], c[1]) / np.sqrt(n_factor))([x_Q, x_KT])\n","    att = L.Lambda(lambda c: K.softmax(c, axis=-1))(res)\n","    att = L.Lambda(lambda c: K.batch_dot(c[0], c[1]))([att, x_V])\n","    return att\n","\n","def multi_head_attention(x, y, n_factor, n_head, dropout):\n","    if n_head == 1:\n","        att = attention(x, y, n_factor, dropout)\n","    else:\n","        n_factor_head = n_factor // n_head\n","        heads = [attention(x, y, n_factor_head, dropout) for i in range(n_head)]\n","        att = L.Concatenate()(heads)\n","        att = L.Dense(n_factor, \n","                      kernel_initializer='glorot_uniform',\n","                      bias_initializer='glorot_uniform',\n","                     )(att)\n","    x = L.Add()([x, att])\n","    x = L.LayerNormalization()(x)\n","    if dropout > 0:\n","        x = L.Dropout(dropout)(x)\n","    return x\n","\n","def res(x, unit, kernel = 3, rate = 0.1):\n","    h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n","    h = L.LayerNormalization()(h)\n","    h = L.LeakyReLU()(h)\n","    h = L.Dropout(rate)(h)\n","    return L.Add()([x, h])\n","\n","def forward(x, unit, kernel = 3, rate = 0.1):\n","    h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n","    h = L.LayerNormalization()(h)\n","    h = L.Dropout(rate)(h)\n","    h = L.LeakyReLU()(h)\n","    h = res(h, unit, kernel, rate)\n","    return h\n","\n","def adj_attn(x, adj, unit, n = 2, rate = 0.1):\n","    x_a = x\n","    x_as = []\n","    for i in range(n):\n","        x_a = forward(x_a, unit)\n","        x_a = tf.matmul(adj, x_a) ## aggregate neighborhoods\n","        x_as.append(x_a)\n","    if n == 1:\n","        x_a = x_as[0]\n","    else:\n","        x_a = L.Concatenate()(x_as)\n","    x_a = forward(x_a, unit)\n","    return x_a\n","\n","\n","def get_base(X_node, adj_matrix):\n","    ## base model architecture \n","    ## node, adj -> middle feature\n","    \n","    node = tf.keras.Input(shape = (None, X_node.shape[2]), name = \"node\")\n","    adj = tf.keras.Input(shape = (None, None, adj_matrix.shape[3]), name = \"adj\")\n","    \n","    adj_learned = L.Dense(1, \"relu\")(adj)\n","    adj_all = L.Concatenate(axis = 3)([adj, adj_learned])\n","        \n","    xs = []\n","    xs.append(node)\n","    x1 = forward(node, 128, kernel = 3, rate = 0.0)\n","    x2 = forward(x1, 64, kernel = 6, rate = 0.0)\n","    x3 = forward(x2, 32, kernel = 15, rate = 0.0)\n","    x4 = forward(x3, 16, kernel = 30, rate = 0.0)\n","    x = L.Concatenate()([x1, x2, x3, x4])\n","    \n","    for unit in [64, 32]:\n","        x_as = []\n","        for i in range(adj_all.shape[3]):\n","            x_a = adj_attn(x, adj_all[:, :, :, i], unit, rate = 0.0)\n","            x_as.append(x_a)\n","        x_c = forward(x, unit, kernel = 30)\n","        \n","        x = L.Concatenate()(x_as + [x_c])\n","        x = forward(x, unit)\n","        x = multi_head_attention(x, x, unit, 4, 0.0)\n","        xs.append(x)\n","        \n","    x = L.Concatenate()(xs)\n","\n","    model = tf.keras.Model(inputs = [node, adj], outputs = [x])\n","    return model\n","\n","\n","def get_ae_model(base, X_node, adj_matrix):\n","    ## denoising auto encoder part\n","    ## node, adj -> middle feature -> node\n","    \n","    node = tf.keras.Input(shape = (None, X_node.shape[2]), name = \"node\")\n","    adj = tf.keras.Input(shape = (None, None, adj_matrix.shape[3]), name = \"adj\")\n","\n","    x = base([L.SpatialDropout1D(0.3)(node), adj])\n","    x = forward(x, 64, rate = 0.3)\n","    p = L.Dense(X_node.shape[2], \"sigmoid\")(x)\n","    \n","    loss = - tf.reduce_mean(20 * node * tf.math.log(p + 1e-4) + (1 - node) * tf.math.log(1 - p + 1e-4))\n","    model = tf.keras.Model(inputs = [node, adj], outputs = [loss])\n","    \n","    opt = get_optimizer()\n","    model.compile(optimizer = opt, loss = lambda t, y : y)\n","    return model\n","\n","\n","# loss functions\n","def MCRMSE(y_true, y_pred):\n","    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=(1))\n","    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=1)\n","\n","def get_model(base, X_node, adj_matrix, seq_len_target):\n","    ## regression part\n","    ## node, adj -> middle feature -> prediction of targets\n","    \n","    node = tf.keras.Input(shape = (None, X_node.shape[2]), name = \"node\")\n","    adj = tf.keras.Input(shape = (None, None, adj_matrix.shape[3]), name = \"adj\")\n","    \n","    x = base([node, adj])\n","    x = forward(x, 128, rate = 0.4)\n","    x = L.Dense(5, None)(x)\n","\n","    model = tf.keras.Model(inputs = [node, adj], outputs = [x])\n","    \n","    opt = get_optimizer()\n","    def mcrmse_loss(t, y, seq_len_target=seq_len_target):\n","        ## calculate mcrmse score by using tf\n","        t = t[:, :seq_len_target]\n","        y = y[:, :seq_len_target]\n","\n","        loss = tf.reduce_mean(tf.sqrt(tf.reduce_mean(tf.reduce_mean((t - y) ** 2, axis = 1), axis = 0)))\n","        return loss\n","    model.compile(optimizer = opt, loss = mcrmse_loss)\n","    return model\n","\n","def get_optimizer():\n","    adam = tf.optimizers.Adam()\n","    return adam"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-03-23T20:24:56.696300Z","iopub.status.busy":"2022-03-23T20:24:56.696005Z","iopub.status.idle":"2022-03-23T20:24:56.704007Z","shell.execute_reply":"2022-03-23T20:24:56.703029Z","shell.execute_reply.started":"2022-03-23T20:24:56.696264Z"},"trusted":true},"outputs":[],"source":["def train_auto_encoder(X_node, adjacency_matrix, epochs, epochs_each, batch_size, save_path):\n","    base = get_base(X_node, adjacency_matrix)\n","    ae_model = get_ae_model(base, X_node, adjacency_matrix)\n","    for i in range(epochs//epochs_each):\n","        print(f\"------ {i} ------\")\n","        ae_model.fit([X_node, adjacency_matrix], [X_node[:,0]],\n","                  epochs = epochs_each,\n","                  batch_size = batch_size)\n","#         ae_model.fit([X_node_pub, As_pub], [X_node_pub[:,0]],\n","#                   epochs = epochs_each,\n","#                   batch_size = batch_size)\n","#         ae_model.fit([X_node_pri, As_pri], [X_node_pri[:,0]],\n","#                   epochs = epochs_each,\n","#                   batch_size = batch_size)\n","        gc.collect()\n","    print(\"****** save ae model ******\")\n","    base.save_weights(save_path)\n","    "]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-03-23T20:25:00.115067Z","iopub.status.busy":"2022-03-23T20:25:00.114503Z","iopub.status.idle":"2022-03-23T20:25:00.922950Z","shell.execute_reply":"2022-03-23T20:25:00.922062Z","shell.execute_reply.started":"2022-03-23T20:25:00.115031Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import KFold\n","def train_gcn(X_node, adjacency_matrix, seq_len_target, epochs, batch_size,\n","              model_path, ae_model_path=None, n_fold=2, validation_frequency=1):\n","    kfold = KFold(n_fold, shuffle = True, random_state = 42)\n","\n","    legends = []\n","    for fold, (tr_idx, va_idx) in enumerate(kfold.split(X_node, adjacency_matrix)):\n","\n","        gc.collect()\n","        tf.keras.backend.clear_session()\n","        print(f'\\nFold - {fold}\\n')\n","        X_node_tr = X_node[tr_idx]\n","        X_node_va = X_node[va_idx]\n","        As_tr = adjacency_matrix[tr_idx]\n","        As_va = adjacency_matrix[va_idx]\n","        y_tr = y[tr_idx]\n","        y_va = y[va_idx]  \n","\n","        base = get_base(X_node, adjacency_matrix)\n","        if ae_model_path:\n","            base.load_weights(ae_model_path)\n","        model = get_model(base, X_node, adjacency_matrix, seq_len_target)\n","        filepath_list = model_path.split('.')\n","        filepath = filepath_list[0] + f'_{fold}' + filepath_list[1]\n","        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","                                    filepath=filepath, save_weights_only=True,\n","                                    monitor='val_loss',mode='min',save_best_only=True)\n","\n","        history = model.fit([X_node_tr, As_tr], [y_tr],\n","                            validation_data=([X_node_va, As_va], [y_va]),\n","                            epochs = epochs,\n","                            batch_size = batch_size, \n","                            validation_freq = validation_frequency,\n","                            callbacks=[model_checkpoint_callback]\n","                       )\n","\n","        plt.plot(history.history['loss'])\n","        plt.plot(history.history['val_loss'])\n","        legends.append(f'loss_fold_{fold}')\n","        legends.append(f'val_loss_fold_{fold}')\n","\n","    plt.title('model loss')\n","    plt.ylabel('loss')\n","    plt.xlabel('epoch')\n","    plt.legend(legends, loc='upper left')\n","    plt.show()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-03-23T20:25:05.000592Z","iopub.status.busy":"2022-03-23T20:25:04.999754Z","iopub.status.idle":"2022-03-23T20:25:55.344382Z","shell.execute_reply":"2022-03-23T20:25:55.343367Z","shell.execute_reply.started":"2022-03-23T20:25:05.000528Z"},"trusted":true},"outputs":[],"source":["denoise = True # if True, use train data whose signal_to_noise > 1\n","\n","path = \"../input/stanfordcovidvaccine\"\n","\n","train = pd.read_json(f\"{path}/train.json\",lines=True)\n","if denoise:\n","    train = train[train.signal_to_noise > 1].reset_index(drop = True)\n","    \n","test  = pd.read_json(f\"{path}/test.json\",lines=True)\n","test_pub = test[test[\"seq_length\"] == 107]\n","test_pri = test[test[\"seq_length\"] == 130]\n","sub = pd.read_csv(f\"{path}/sample_submission.csv\")\n","\n","targets = list(sub.columns[1:])\n","print(targets)\n","\n","y_train = []\n","seq_len = train[\"seq_length\"].iloc[0]\n","seq_len_target = train[\"seq_scored\"].iloc[0]\n","ignore = -10000\n","ignore_length = seq_len - seq_len_target\n","for target in targets:\n","    y = np.vstack(train[target])\n","    dummy = np.zeros([y.shape[0], ignore_length]) + ignore\n","    y = np.hstack([y, dummy])\n","    y_train.append(y)\n","y = np.stack(y_train, axis = 2)\n","y.shape\n","\n","adjacency_matrix = get_adj_matrix(train)\n","adjacency_matrix_pub = get_adj_matrix(test_pub)\n","adjacency_matrix_pri = get_adj_matrix(test_pri)\n","\n","X_node = get_node_features(train)\n","X_node_pub = get_node_features(test_pub)\n","X_node_pri = get_node_features(test_pri)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-23T20:25:58.779887Z","iopub.status.busy":"2022-03-23T20:25:58.779535Z"},"trusted":true},"outputs":[],"source":["\n","epochs_list = [30, 10, 3, 3, 5, 5]\n","batch_size_list = [8, 16, 32, 64, 128, 256] \n","\n","epochs = epochs_list[0]\n","batch_size = batch_size_list[1]\n","\n","model_path = \"./model_without_ae.h5\"\n","train_gcn(X_node, adjacency_matrix, seq_len_target, epochs, batch_size,\n","              model_path)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-03-23T19:52:48.232383Z","iopub.status.busy":"2022-03-23T19:52:48.231617Z","iopub.status.idle":"2022-03-23T19:55:35.628270Z","shell.execute_reply":"2022-03-23T19:55:35.626865Z","shell.execute_reply.started":"2022-03-23T19:52:48.232346Z"},"trusted":true},"outputs":[],"source":["ae_epochs = 20 # epoch of training of denoising auto encoder\n","ae_epochs_each = 5 # epoch of training of denoising auto encoder each time.                  \n","ae_batch_size = 32\n","ae_path = \"./base_ae\"\n","train_auto_encoder(X_node, adjacency_matrix, ae_epochs, ae_epochs_each, \n","                   ae_batch_size, ae_path)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-03-23T19:57:19.849251Z","iopub.status.busy":"2022-03-23T19:57:19.848920Z","iopub.status.idle":"2022-03-23T19:57:25.406604Z","shell.execute_reply":"2022-03-23T19:57:25.405087Z","shell.execute_reply.started":"2022-03-23T19:57:19.849218Z"},"trusted":true},"outputs":[],"source":["epochs_list = [30, 10, 3, 3, 5, 5]\n","batch_size_list = [8, 16, 32, 64, 128, 256] \n","\n","epochs = epochs_list[0]\n","batch_size = batch_size_list[1]\n","\n","model_path = \"./model_with_ae.h5\"\n","train_gcn(X_node, adjacency_matrix, seq_len_target, epochs, batch_size,\n","              model_path, ae_model_path=ae_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","\n","p_pub = 0\n","p_pri = 0\n","base = get_base(X_node, adjacency_matrix_pub)\n","model = get_model(base, X_node, adjacency_matrix_pub, seq_len_target)\n","for i in range(2):\n","    model.load_weights(f'./model_with_ae_{i}.h5')\n","    p_pub += model.predict([X_node_pub, adjacency_matrix_pub]) / 5\n","    p_pri += model.predict([X_node_pri, adjacency_matrix_pri]) / 5\n","    if one_fold:\n","        p_pub *= 5\n","        p_pri *= 5\n","        break\n","\n","for i, target in enumerate(targets):\n","    test_pub[target] = [list(p_pub[k, :, i]) for k in range(p_pub.shape[0])]\n","    test_pri[target] = [list(p_pri[k, :, i]) for k in range(p_pri.shape[0])]\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
